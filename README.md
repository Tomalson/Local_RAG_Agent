# ğŸ¤– Local RAG Agent

> **Profesjonalny system Retrieval-Augmented Generation z lokalnymi dokumentami**
> 
> Inteligentny asystent AI oparty na LLaMA 3 z lokalnÄ… bazÄ… wektorowÄ… ChromaDB.
> Nie wysyÅ‚aj danych do chmury â€” wszystko dziaÅ‚a lokalnie! ğŸ”’

---

## âœ¨ Cechy

- âœ… **CaÅ‚kowicie lokalny** - bez wysyÅ‚ania danych do chmury
- âœ… **Szybki** - wyszukiwanie wektorowe z ChromaDB
- âœ… **Inteligentny** - LLaMA 3 z Query Decomposition i Hybrid Search
- âœ… **Elastyczny** - obsÅ‚uguje PDF i Markdown
- âœ… **Åatwy w konfiguracji** - jeden plik `config.py`
- âœ… **Bez API** - nie potrzebujesz klucza OpenAI

---

## ğŸš€ Szybki Start (5 minut)

### 1. Wymagania
- **Python 3.10+**
- **Ollama** (https://ollama.ai) z modelami:
  ```bash
  ollama pull llama3              # Model QA/generacyjny
  ollama pull nomic-embed-text    # Model embeddingÃ³w
  ```

### 2. Instalacja
```bash
cd Local_RAG_Agent
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # macOS/Linux

pip install -r requirements.txt
```

### 3. Dodaj dokumenty
UmieÅ›Ä‡ pliki w `Local_RAG_Agent/docs/`:
- **PDF**: `*.pdf`
- **Markdown**: `*.md`

### 4. Zbuduj bazÄ™ wektorowÄ…
```bash
# Markdown
python ingest_md.py

# PDF
python ingest.py
```

### 5. Zadawaj pytania
```bash
python main.py
```

**PrzykÅ‚ad:**
```
â“ Pytanie: Opisz gÅ‚Ã³wne cechy systemu
ğŸ’¡ ODPOWIEDÅ¹: [OdpowiedÅº z dokumentÃ³w]
ğŸ“š Å¹RÃ“DÅA:
   â€¢ document1.md
   â€¢ document2.pdf
```

---

## ğŸ“‹ PeÅ‚na Dokumentacja

### Struktura projektu
```
Local_RAG_Agent/
â”œâ”€â”€ docs/              # ğŸ“ WejÅ›ciowe dokumenty (PDF, MD)
â”œâ”€â”€ chroma_db/         # ğŸ—„ï¸ Baza wektorowa (utworzana automatycznie)
â”œâ”€â”€ config.py          # âš™ï¸ Konfiguracja (modele, Å›cieÅ¼ki, prompty)
â”œâ”€â”€ main.py            # ğŸ¯ CLI interfejs (main.py)
â”œâ”€â”€ advanced_rag.py    # ğŸ§  Silnik RAG (Query Decomposition, Hybrid Search)
â”œâ”€â”€ rag_service.py     # ğŸ”§ Niskopoziomowe API
â”œâ”€â”€ ingest_md.py       # ğŸ“„ Wczytywanie Markdown
â”œâ”€â”€ ingest.py          # ğŸ“‹ Wczytywanie PDF
â”œâ”€â”€ requirements.txt   # ğŸ“¦ ZaleÅ¼noÅ›ci
â””â”€â”€ run.bat           # ğŸš€ Skrypt uruchamiajÄ…cy (Windows)
```

### Konfiguracja

Plik [Local_RAG_Agent/config.py](Local_RAG_Agent/config.py) zawiera:

| Ustawienie | WartoÅ›Ä‡ | Opis |
|-----------|---------|------|
| `LLM_MODEL` | `llama3` | Model generacyjny |
| `EMBEDDING_MODEL` | `nomic-embed-text` | Model embeddingÃ³w |
| `RETRIEVER_K` | `8` | Liczba fragmentÃ³w do retrieve'u |
| `CHUNK_SIZE` | `700` | Rozmiar fragmentu w znakach |
| `LLM_TEMPERATURE` | `0.1` | Temperatura (niÅ¼ej = bardziej deterministyczne) |

**Zmiana modelu LLaMA:**
```python
LLM_MODEL = "llama2"  # lub inne dostÄ™pne modele
```

DostÄ™pne modele: `ollama list`

### Skrypty

#### `main.py` - Interaktywny interfejs
```bash
python main.py
```
- Pytania + odpowiedzi w CLI
- Automatyczne odkrywanie ÅºrÃ³deÅ‚
- Statystyki bazy (`stats` komenda)

#### `ingest_md.py` - Wczytywanie Markdown
```bash
python ingest_md.py
```
- Skanuje `docs/*.md`
- Tworzy embeddingi
- Dodaje do ChromaDB

#### `ingest.py` - Wczytywanie PDF
```bash
python ingest.py
```
- Skanuje `docs/*.pdf`
- Ekstrakcja tekstu
- Tworzy embeddingi
- Dodaje do ChromaDB

---

## âš™ï¸ Zaawansowana Konfiguracja

### Query Decomposition
Agent automatycznie rozbija zÅ‚oÅ¼one pytania:
```
Oryginalnie pytanie:
  "Jak zainstalowaÄ‡ i konfigurowaÄ‡ system?"
â†“
RozÅ‚oÅ¼one na:
  1. "Jak zainstalowaÄ‡ system?"
  2. "Jak skonfigurowaÄ‡ system?"
  3. "Jakie sÄ… wymagania systemowe?"
```

### Hybrid Search
Kombinacja:
- **Lexical Search** - dopasowanie sÅ‚Ã³w kluczowych
- **Semantic Search** - dopasowanie znaczenia (wektory)
- **MMR (Maximum Marginal Relevance)** - dywersyfikacja wynikÃ³w

### Context Expansion
Automatyczne powiÄ™kszanie kontekstu:
- Wyszukiwanie sÄ…siednich fragmentÃ³w
- Rozszeranie okna kontekstu
- Lepsze ÅºrÃ³dÅ‚a dla LLM

---

## ğŸ†˜ Troubleshooting

### Ollama Connection Refused
```bash
# Sprawdzenie czy Ollama dziaÅ‚a
ollama list

# JeÅ›li nie dziaÅ‚a:
# 1. Zamknij aplikacjÄ™ Ollama
# 2. Uruchom ponownie
# 3. SprÃ³buj jeszcze raz
```

### Brak modelu
```bash
ollama pull llama3
ollama pull nomic-embed-text
```

### CUDA/Memory Error
```python
# config.py - zmieÅ„ na CPU-friendly model
EMBEDDING_MODEL = "nomic-embed-text"  # Lekki model
LLM_TEMPERATURE = 0.1  # Mniej hallucynacji
```

### WyczyÅ›Ä‡ bazÄ™ i zaÅ‚aduj od nowa
```bash
# 1. UsuÅ„ starÄ… bazÄ™
rm -r Local_RAG_Agent/chroma_db

# 2. ZaÅ‚aduj dokumenty ponownie
python ingest_md.py
# lub
python ingest.py
```

### Dokumenty nie sÄ… wczytywane
- SprawdÅº czy pliki sÄ… w `Local_RAG_Agent/docs/`
- SprawdÅº czy rozszerzenia to `.md` lub `.pdf`
- SprÃ³buj: `python ingest_md.py -v` (verbose mode)

---

## ğŸ—ï¸ Architektura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      main.py (CLI)              â”‚
â”‚  Interactive Q&A Interface      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    advanced_rag.py              â”‚
â”‚  AdvancedRAGAgent               â”‚
â”‚  â”œâ”€ Query Decomposition         â”‚
â”‚  â”œâ”€ Hybrid Search               â”‚
â”‚  â””â”€ Context Expansion           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    rag_service.py               â”‚
â”‚  RAG Service (Low-level)        â”‚
â”‚  â”œâ”€ Vector Store (ChromaDB)     â”‚
â”‚  â”œâ”€ Embeddings (Ollama)         â”‚
â”‚  â””â”€ LLM (Ollama)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama (Local LLM)             â”‚
â”‚  - LLaMA 3 (LLM)                â”‚
â”‚  - Nomic Embed (Embeddings)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š PrzykÅ‚ady UÅ¼ycia

### PrzykÅ‚ad 1: Dokumentacja API
```bash
# 1. UmieÅ›Ä‡ dokumentacjÄ™ w docs/
cp project-docs/*.md Local_RAG_Agent/docs/

# 2. ZaÅ‚aduj
python ingest_md.py

# 3. Pytaj
python main.py
â“ Pytanie: Jak siÄ™ uwierzytelniaÄ‡ w API?
```

### PrzykÅ‚ad 2: Dokumenty PDF
```bash
# 1. UmieÅ›Ä‡ PDF
cp reports/*.pdf Local_RAG_Agent/docs/

# 2. ZaÅ‚aduj
python ingest.py

# 3. Pytaj
python main.py
â“ Pytanie: Jaki byÅ‚ przychÃ³d w Q4 2024?
```

### PrzykÅ‚ad 3: Mieszane ÅºrÃ³dÅ‚a
```bash
# Masz zarÃ³wno MD jak i PDF
python ingest_md.py  # wczyta *.md
python ingest.py     # wczyta *.pdf
python main.py       # pyta z obu
```

---

## ğŸ”’ BezpieczeÅ„stwo i PrywatnoÅ›Ä‡

âœ… **Wszystko lokalnie**
- Nie wysyÅ‚amy danych do chmury
- Nie ma zaufania do Å¼adnych API
- PeÅ‚na kontrola nad danymi

âœ… **Bez API Keys**
- Nie potrzebujesz OpenAI, Claude, itp.
- Brak ryzyka wyciekÃ³w kluczy

âœ… **GDPR Compliant**
- Dane nigdy nie opuszczajÄ… Twojego komputera
- Idealne dla poufnych dokumentÃ³w

---

## ğŸ“– Zasoby

- **Ollama**: https://ollama.ai
- **ChromaDB**: https://docs.trychroma.com
- **LangChain**: https://python.langchain.com
- **LLaMA**: https://llama.meta.com

---

## ğŸ“ Licencja

MIT License - patrz [LICENSE](LICENSE)

---

## ğŸ¤ WkÅ‚ad

ZnaleÅºliÅ›cie bug? Macie ideÄ™?
- OtwÃ³rzcie Issue ğŸ›
- WyÅ›lijcie Pull Request ğŸš€

---

## â“ FAQ

**P: Czy mogÄ™ uÅ¼ywaÄ‡ inne modele?**
A: Tak! ZmieÅ„ `LLM_MODEL` w `config.py`. Lista: `ollama list`

**P: Jaka jest minimalna RAM?**
A: ~4GB dla LLaMA 3. WiÄ™cej RAM = szybciej.

**P: Czy dziaÅ‚a na GPU?**
A: Tak! Ollama automatycznie uÅ¼yje GPU jeÅ›li jest dostÄ™pne.

**P: Czy mogÄ™ uÅ¼yÄ‡ wiÄ™cej niÅ¼ jednej kolekcji dokumentÃ³w?**
A: Tak, w `config.py` zmieÅ„ `CHROMA_COLLECTION_NAME`.

**P: Jak szybko sÄ… odpowiedzi?**
A: 5-15 sekund zaleÅ¼y od GPU, CPU i rozmiaru kontekstu.

---

**Czekamy na Ciebie! ğŸš€ JeÅ›li podoba Ci siÄ™ projekt, daj â­**

Ostatnia aktualizacja: 2026-01-15
