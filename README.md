# ğŸ¤– Local RAG Agent

> **Lokalny system RAG do analizy dokumentÃ³w PDF**
> 
> Zadawaj pytania swoim dokumentom PDF uÅ¼ywajÄ…c lokalnego AI (Ollama + LLaMA 3).
> Wszystko dziaÅ‚a na Twoim komputerze - Å¼adnych danych w chmurze! ğŸ”’

## âš ï¸ WaÅ¼ne

To narzÄ™dzie uÅ¼ywa **darmowej wersji Ollama z modelem LLaMA 3**, ktÃ³ry dziaÅ‚a lokalnie na Twoim komputerze. 
**To nie jest ChatGPT ani Gemini** - odpowiedzi sÄ… wolniejsze i mniej precyzyjne, ale wszystko pozostaje prywatne.

---

## âœ¨ Co robi?

- ğŸ“„ **GÅ‚Ã³wnie czyta PDF-y** - dokumenty, raporty, ksiÄ…Å¼ki
- ğŸ“ **Dodatkowo obsÅ‚uguje Markdown** - pliki .md jako bonus
- ğŸ’¬ Zadajesz pytanie â†’ dostaniesz odpowiedÅº z dokumentÃ³w
- ğŸ” Pokazuje ÅºrÃ³dÅ‚a - skÄ…d wziÄ™Å‚a siÄ™ odpowiedÅº
- ğŸ”’ **100% prywatne** - Å¼adne dane nie wychodzÄ… z komputera

---

## ğŸš€ Szybki Start

### 1. Zainstaluj Ollama
Pobierz z https://ollama.ai i zainstaluj modele:
```bash
ollama pull llama3              # GÅ‚Ã³wny model AI
ollama pull nomic-embed-text    # Model do embeddingÃ³w
```

### 2. Zainstaluj zaleÅ¼noÅ›ci Python
```bash
cd Local_RAG_Agent
pip install -r requirements.txt
```

### 3. WrzuÄ‡ swoje PDF-y
Skopiuj pliki do `Local_RAG_Agent/docs/`:
- **PDF** - gÅ‚Ã³wny format (dokumenty, raporty, ksiÄ…Å¼ki)
- **MD** - opcjonalnie pliki markdown

### 4. ZaÅ‚aduj dokumenty do systemu
```bash
python ingest.py         # Dla PDF
python ingest_md.py      # Dla Markdown (opcjonalnie)
```

### 5. Zadawaj pytania!
```bash
python main.py
```

**PrzykÅ‚ad:**
```
â“ Pytanie: O czym jest dokument?
ğŸ’¡ ODPOWIEDÅ¹: [OdpowiedÅº na podstawie PDF-Ã³w]
ğŸ“š Å¹RÃ“DÅA: document.pdf
```

---

## ğŸ“ Struktura
```
Local_RAG_Agent/
â”œâ”€â”€ docs/              # Tu wrzucasz PDF-y i MD
â”œâ”€â”€ chroma_db/         # Baza wektorowa (tworzy siÄ™ sama)
â”œâ”€â”€ main.py            # Program do zadawania pytaÅ„
â”œâ”€â”€ ingest.py          # Wczytywanie PDF
â”œâ”€â”€ ingest_md.py       # Wczytywanie MD (opcja)
â”œâ”€â”€ config.py          # Ustawienia
â””â”€â”€ requirements.txt   # ZaleÅ¼noÅ›ci Python
```

---

## âš™ï¸ Podstawowa Konfiguracja

W [Local_RAG_Agent/config.py](Local_RAG_Agent/config.py) moÅ¼esz zmieniÄ‡:

- `LLM_MODEL` - model AI (domyÅ›lnie `llama3`)
- `CHUNK_SIZE` - rozmiar fragmentÃ³w tekstu (domyÅ›lnie 700)
- `RETRIEVER_K` - ile fragmentÃ³w wyszukiwaÄ‡ (domyÅ›lnie 8)

**DostÄ™pne modele:**
```bash
ollama list  # Zobacz zainstalowane modele
```

## ğŸ†˜ NajczÄ™stsze Problemy

### Ollama nie dziaÅ‚a
```bash
ollama list  # SprawdÅº czy dziaÅ‚a
# JeÅ›li nie - uruchom ponownie aplikacjÄ™ Ollama
```

### Brak modelu
```bash
ollama pull llama3
ollama pull nomic-embed-text
```

### Reset bazy danych
```bash
rm -r Local_RAG_Agent/chroma_db  # UsuÅ„ bazÄ™
python ingest.py                  # ZaÅ‚aduj ponownie
```

---

## â“ FAQ

**P: Dlaczego odpowiedzi sÄ… wolne?**
A: Ollama dziaÅ‚a lokalnie na Twoim CPU/GPU. To nie jest ChatGPT w chmurze - wymaga wiÄ™cej czasu, ale zachowuje prywatnoÅ›Ä‡.

**P: Czy mogÄ™ uÅ¼ywaÄ‡ innych modeli?**
A: Tak! Zobacz dostÄ™pne: `ollama list`. ZmieÅ„ w `config.py`.

**P: Ile RAM potrzebujÄ™?**
A: Minimum 4-8GB. LLaMA 3 jest doÅ›Ä‡ wymagajÄ…cy.

**P: Dlaczego odpowiedzi sÄ… mniej dokÅ‚adne niÅ¼ ChatGPT?**
A: LLaMA 3 (szczegÃ³lnie w wersji lokalnej) ma swoje ograniczenia. Jest mniejszy i dziaÅ‚a offline, wiÄ™c nie bÄ™dzie tak precyzyjny jak duÅ¼e modele komercyjne.

**P: Czy obsÅ‚uguje tylko PDF?**
A: GÅ‚Ã³wnie PDF (to priorytet), ale dodatkowo moÅ¼e teÅ¼ czytaÄ‡ pliki Markdown (.md).

---

## ğŸ“– Technologie

- **Ollama** - Lokalne modele AI
- **LLaMA 3** - Model jÄ™zykowy
- **ChromaDB** - Baza wektorowa
- **LangChain** - Framework RAG
- **PDFPlumberLoader** - Czytanie PDF

---

**Made for privacy**
