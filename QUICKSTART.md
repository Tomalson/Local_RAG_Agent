# ðŸš€ QUICK START â€” Tylko Local RAG Agent (bez scrapera)

## ðŸ“‹ Wymagania

```bash
# Python 3.10+
python --version

# Ollama
# Pobierz: https://ollama.ai
ollama pull llama3
ollama pull nomic-embed-text   # embedding model
```

Pracujemy wyÅ‚Ä…cznie w folderze `Local_RAG_Agent/`.

## Krok 1: Instalacja zaleÅ¼noÅ›ci

```bash
cd Local_RAG_Agent
..\.venv\Scripts\python.exe -m pip install -r requirements.txt
```

## Krok 2: Dodaj dokumenty

- Markdown: wrzuÄ‡ `.md` do `docs/`
- PDF: wrzuÄ‡ `.pdf` do `docs/`

## Krok 3: Zbuduj bazÄ™

```bash
# Dla Markdown
..\.venv\Scripts\python.exe ingest_md.py

# Dla PDF
..\.venv\Scripts\python.exe ingest.py
```

## Krok 4: Zapytaj RAG

```bash
..\.venv\Scripts\python.exe main.py
```

PrzykÅ‚ad interfejsu:
```
â“ Twoje pytanie: Opisz proces instalacji
ðŸ’¡ ODPOWIEDÅ¹: [z kontekstu]
ðŸ“š Å¹RÃ“DÅA:
  1. docs_1.md
```

## ðŸ”§ Konfiguracja kluczowa

Plik [Local_RAG_Agent/config.py](Local_RAG_Agent/config.py):
- `EMBEDDING_MODEL = "nomic-embed-text"`
- `LLM_MODEL = "llama3"`
- `RETRIEVER_K = 4`, `LLM_TEMPERATURE = 0.1`
- Katalog bazy: `chroma_db/`

## ðŸ†˜ Szybkie naprawy

- Ollama nie odpowiada / connection refused:
  - Zamknij i uruchom ponownie aplikacjÄ™ Ollama, potem `ollama list`.
- Brak embeddingÃ³w / CUDA error na llama3:
  - UÅ¼ywaj `nomic-embed-text` (juÅ¼ ustawione w config).
- Chcesz wyczyÅ›ciÄ‡ bazÄ™ i wgraÄ‡ od nowa:
  - UsuÅ„ zawartoÅ›Ä‡ `chroma_db/` i ponownie uruchom `ingest_md.py` lub `ingest.py`.

## ðŸŽ¯ Minimalny demo

```bash
cd Local_RAG_Agent
echo "# Test\nTo jest testowy dokument" > docs/test.md
..\.venv\Scripts\python.exe ingest_md.py
..\.venv\Scripts\python.exe main.py
```

To wszystko â€” RAG bez scrapera. ðŸš€
